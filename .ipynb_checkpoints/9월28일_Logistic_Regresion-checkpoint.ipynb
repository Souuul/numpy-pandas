{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9월 28일 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다변수 수치미분코드\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def numerical_derivative(f,x):\n",
    "    \n",
    "    # f : 미분하려고 하는 다변수 함수\n",
    "    # x : 모든 변수를 포함하고 있는 numpy array(차원상관없음)\n",
    "    \n",
    "    delta_x = 1e-4\n",
    "    derivative_x = np.zeros_like(x)  # 계산된 수치미분 값을 저장하기 위한 변수\n",
    "    \n",
    "    # iterator를 이용하여 입력변수 x에 대해 편미분 수행\n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        \n",
    "        idx = it.multi_index   # 현재의 index를 추출 => tuple형태로 리턴\n",
    "        \n",
    "        tmp = x[idx]           # 현재 idx의 값을 잠시 보존. delta_x를 이용한 값으로\n",
    "                               # ndarray를 수정한 후 함수값을 계산해야 하기 때문\n",
    "                               # 함수값을 계산한 후 원상복구해야 다음 변수에 대한 편미분을\n",
    "                               # 정상적으로 수행할 수 있다.\n",
    "        \n",
    "        x[idx] = tmp + delta_x\n",
    "        fx_plus_delta = f(x)   # f(x + delta_x)\n",
    "        \n",
    "        x[idx] = tmp - delta_x\n",
    "        fx_minus_delta = f(x)   # f(x - delta_x)\n",
    "        \n",
    "        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)\n",
    "    \n",
    "        x[idx] = tmp\n",
    "        \n",
    "        it.iternext()\n",
    "        \n",
    "    return derivative_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/envs/data_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/Users/admin/opt/anaconda3/envs/data_env/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### python 결과값 #########\n",
      "공부시간 : [[13]], 결과 : (1, array([[0.58068398]]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Logistic Regression을 python, tensorflow, sklearn으로 각각구현해 보아요!\n",
    "# 처음은 독립변수가 1개인 걸로 가요!!\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "\n",
    "# 수치미분함수를 들고와서 사용해요!\n",
    "# ##################################\n",
    "# 다변수 수치미분코드\n",
    "\n",
    "def numerical_derivative(f,x):\n",
    "    \n",
    "    # f : 미분하려고 하는 다변수 함수\n",
    "    # x : 모든 변수를 포함하고 있는 numpy array(차원상관없음)\n",
    "    \n",
    "    delta_x = 1e-4\n",
    "    derivative_x = np.zeros_like(x)  # 계산된 수치미분 값을 저장하기 위한 변수\n",
    "    \n",
    "    # iterator를 이용하여 입력변수 x에 대해 편미분 수행\n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        \n",
    "        idx = it.multi_index   # 현재의 index를 추출 => tuple형태로 리턴\n",
    "        \n",
    "        tmp = x[idx]           # 현재 idx의 값을 잠시 보존. delta_x를 이용한 값으로\n",
    "                               # ndarray를 수정한 후 함수값을 계산해야 하기 때문\n",
    "                               # 함수값을 계산한 후 원상복구해야 다음 변수에 대한 편미분을\n",
    "                               # 정상적으로 수행할 수 있다.\n",
    "        \n",
    "        x[idx] = tmp + delta_x\n",
    "        fx_plus_delta = f(x)   # f(x + delta_x)\n",
    "        \n",
    "        x[idx] = tmp - delta_x\n",
    "        fx_minus_delta = f(x)   # f(x - delta_x)\n",
    "        \n",
    "        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)\n",
    "    \n",
    "        x[idx] = tmp\n",
    "        \n",
    "        it.iternext()\n",
    "        \n",
    "    return derivative_x\n",
    "\n",
    "# ##################################\n",
    "\n",
    "# Raw Data Loading + Data Preprocessing\n",
    "# 그런데 이번예제는 이 과정이 필요없죠!!\n",
    "\n",
    "# Training Data Set\n",
    "# 지도학습을 하고 있기 때문에 독립변수와 종속변수(label)로 구분해서 데이터를 준비\n",
    "# 어떤경우에는 이 두개를 아예 분리해서 제공하는 경우도 있어요!\n",
    "x_data = np.arange(2,21,2).reshape(-1,1)\n",
    "t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)\n",
    "\n",
    "#########################################################\n",
    "# python 구현부터 해 보아요!!\n",
    "\n",
    "# Weight & bias     \n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)   # (1,)\n",
    "\n",
    "# 위에서 정의한 W와 b의 값을 구해야 해요!\n",
    "# 이 값만 구하면 우리의 최종 목적인 model을 완성할 수 있어요!\n",
    "\n",
    "# loss function(손실함수, cost function , 비용함수)\n",
    "# 우리 모델의 예측값과 들어온 t_data(정답)\n",
    "# 입력으로 들어온 x_data와 W,b값을 이용해서 예측값 계산\n",
    "# t_data(정답)을 비교해되요!!\n",
    "def loss_func(input_obj):\n",
    "    \n",
    "    # input_obj : W와 b를 같이 포함하고 있는 ndarray => [W1 W2 W3 b]\n",
    "    num_of_bias = b.shape[0]   # num_of_bias : 1\n",
    "    \n",
    "    input_W = input_obj[:-1*num_of_bias].reshape(-1,num_of_bias)   # 행렬연산을 하기 위한 W를 생성\n",
    "    input_b = input_obj[-1*num_of_bias:]                           # bias\n",
    "    \n",
    "    \n",
    "    #  우리 모델의 예측값 : (linear regression model(Wx + b) ==> sigmoid를 적용 )\n",
    "    z = np.dot(x_data,input_W) + input_b\n",
    "    y = 1 / ( 1 + np.exp(-1 * z) )  # sigmoid\n",
    "    \n",
    "    delta = 1e-7  #  굉장히 작은값을 이용해서 프로그램으로 \n",
    "                  # 로그 연산시 무한대로 발산하는것을 방지\n",
    "        \n",
    "    # cross entropy\n",
    "    return -np.sum(t_data*np.log(y+delta) + ((1-t_data)*np.log(1-y+delta)))\n",
    "    \n",
    "# learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    \n",
    "    input_param = np.concatenate((W.ravel(), b.ravel()),axis=0)   # [W1 W2 W3 b]\n",
    "    derivative_result = learning_rate* numerical_derivative(loss_func,input_param)\n",
    "\n",
    "    num_of_bias = b.shape[0] \n",
    "    \n",
    "    W = W - derivative_result[:-1*num_of_bias].reshape(-1,num_of_bias)   # [[W1] [W2] [W3]]\n",
    "    b = b - derivative_result[-1*num_of_bias:]\n",
    "    \n",
    "    \n",
    "# predict => W,b를 다 구해서!! 우리의 Logistic Regression Model을 완성!!\n",
    "def logistic_predict(x):  # 공부한 시간이 입력으로 들어와요!!\n",
    "    \n",
    "    z = np.dot(x,W) + b\n",
    "    y = 1 / ( 1 + np.exp(-1*z) )\n",
    "    \n",
    "    if y < 0.5:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = 1\n",
    "        \n",
    "    return result, y\n",
    "\n",
    "study_hour = np.array([[13]])\n",
    "result = logistic_predict(study_hour)\n",
    "print('####### python 결과값 #########')\n",
    "print('공부시간 : {}, 결과 : {}'.format(study_hour,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### sklearn 결과값 #########\n",
      "공부시간 : [[13]], 결과 : [0],[[0.50009391 0.49990609]]\n"
     ]
    }
   ],
   "source": [
    "### sklearn ### 으로 구현해보아요!\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "# Logistic Regression Model을 생성해요!\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "# Training data set을 이용해서 학습\n",
    "model.fit(x_data,t_data.ravel())\n",
    "\n",
    "study_hour = np.array([[13]])\n",
    "predict_val = model.predict(study_hour)\n",
    "predict_proba = model.predict_proba(study_hour)\n",
    "print('####### sklearn 결과값 #########')\n",
    "print('공부시간 : {}, 결과 : {},{}'.format(study_hour,predict_val,predict_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### tensorflow 결과값 #########\n",
      "공부시간 : [13], 결과 : [[0.5767814]]\n"
     ]
    }
   ],
   "source": [
    "# tensorflow를 이용한 구현\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(dtype=tf.float32)   # 독립변수가 1개인경우 shape명시하지 않아요! (x_data)\n",
    "T = tf.placeholder(dtype=tf.float32)   # (t_data)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([1,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = W * X + b  # matrix 곱연산 하지 않나요?? \n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    sess.run(train, feed_dict={X:x_data, T:t_data})\n",
    "\n",
    "\n",
    "study_hour = np.array([13])\n",
    "result = sess.run(H,feed_dict={X:study_hour})\n",
    "print('####### tensorflow 결과값 #########')\n",
    "print('공부시간 : {}, 결과 : {}'.format(study_hour,result))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Variable Logistic Regression\n",
    "# 독립변수가 2개 이상인 Logistic Regression\n",
    "\n",
    "# 학습하는 데이터는 GRE(Graduate Record Examination)와 \n",
    "# GPA(Grade Point Average) 성적 그리고 \n",
    "# Rank(University Rating)에 대한 \n",
    "# 대학원 합격/불합격 정보\n",
    "\n",
    "# 내 성적 [600.    3.8   1. ] 의 결과??\n",
    "# 첫번째 구현은 sklearn으로 하세요!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### sklearn으로 구현한 결과 ####\n",
      "[[600.    3.8   1. ]] [1] [[0.43740782 0.56259218]]\n",
      "W : [[ 0.7325504]\n",
      " [ 0.8170243]\n",
      " [-0.1877611]], b : [-0.59476596], loss : 0.701562225818634\n",
      "W : [[ 0.5626053 ]\n",
      " [ 0.63225025]\n",
      " [-0.43898708]], b : [-0.9528125], loss : 0.60415118932724\n",
      "W : [[ 0.5394202]\n",
      " [ 0.6050528]\n",
      " [-0.5519007]], b : [-1.0485609], loss : 0.595654308795929\n",
      "W : [[ 0.5527239 ]\n",
      " [ 0.61714506]\n",
      " [-0.62712765]], b : [-1.0774772], loss : 0.5933252573013306\n",
      "W : [[ 0.57495624]\n",
      " [ 0.63902646]\n",
      " [-0.6892492 ]], b : [-1.0876971], loss : 0.5916750431060791\n",
      "W : [[ 0.59820205]\n",
      " [ 0.6622723 ]\n",
      " [-0.7450449 ]], b : [-1.0923618], loss : 0.5902623534202576\n",
      "W : [[ 0.62144786]\n",
      " [ 0.6853698 ]\n",
      " [-0.79677933]], b : [-1.0959381], loss : 0.5890110731124878\n",
      "W : [[ 0.64302546]\n",
      " [ 0.70682746]\n",
      " [-0.8449825 ]], b : [-1.0995144], loss : 0.5879172086715698\n",
      "W : [[ 0.6640662 ]\n",
      " [ 0.72769547]\n",
      " [-0.89013964]], b : [-1.1030906], loss : 0.5869429707527161\n",
      "W : [[ 0.6837357 ]\n",
      " [ 0.747365  ]\n",
      " [-0.93225104]], b : [-1.1066669], loss : 0.5860869884490967\n",
      "Accuracy : 0.3219895362854004\n"
     ]
    }
   ],
   "source": [
    "# %reset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/admission.csv')\n",
    "\n",
    "# 결측치 확인\n",
    "# df.isnull().sum() 결측치는 없네~\n",
    "\n",
    "# 이상치를 확인해서 있으면 제거!\n",
    "# fig = plt.figure()\n",
    "# fig_admit = fig.add_subplot(1,4,1)\n",
    "# fig_gre = fig.add_subplot(1,4,2)\n",
    "# fig_gpa = fig.add_subplot(1,4,3)\n",
    "# fig_rank = fig.add_subplot(1,4,4)\n",
    "\n",
    "# fig_admit.boxplot(df['admit'])\n",
    "# fig_gre.boxplot(df['gre'])\n",
    "# fig_gpa.boxplot(df['gpa'])\n",
    "# fig_rank.boxplot(df['rank'])\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# 확인했더니 이상치가 있어요!\n",
    "# 이상치를 제거해요!!\n",
    "zscore_threshold = 2.0\n",
    "\n",
    "for col in df.columns:\n",
    "    outlier = df[col][np.abs(stats.zscore(df[col])) > zscore_threshold]    \n",
    "    df = df.loc[~df[col].isin(outlier)]\n",
    "    \n",
    "\n",
    "# Training Data Set\n",
    "x_data = df.drop('admit', axis=1, inplace=False).values\n",
    "t_data = df['admit'].values.reshape(-1,1)\n",
    "\n",
    "# 정규화를 진행해야 해요!!\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(x_data)\n",
    "norm_x_data = scaler_x.transform(x_data)  # for python, tensorflow\n",
    "\n",
    "# sklearn을 이용한 구현\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(x_data,t_data.ravel())\n",
    "print('#### sklearn으로 구현한 결과 ####')\n",
    "my_score = np.array([[600, 3.8,1]])\n",
    "predict_val = model.predict(my_score)  # 0 or 1\n",
    "predict_proba = model.predict_proba(my_score)  # (불합격할 확률, 합격할 확률)\n",
    "print(my_score, predict_val, predict_proba)\n",
    "\n",
    "# Tensorflow\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32)  # 독립변수의 데이터를 받기위한 placeholder\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)  # 종속변수(label)의 데이터를 받기위한 placeholder\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)\n",
    "\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(300000):\n",
    "    _, W_val, b_val, loss_val = sess.run([train, W, b, loss], \n",
    "                                         feed_dict={X:norm_x_data, T:t_data})\n",
    "    \n",
    "    if step % 30000 == 0:\n",
    "        print('W : {}, b : {}, loss : {}'.format(W_val, b_val, loss_val))\n",
    "\n",
    "        \n",
    "        \n",
    "# 정확도(Accuracy) 측정\n",
    "predict = tf.cast(H>=-0.5, dtype = tf.float32) # True 1.0 False\n",
    "correct = tf.equal(predict,T) \n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "\n",
    "accuracy_val = sess.run(accuracy, feed_dict = {X:x_data, T:t_data})\n",
    "print('Accuracy : {}'.format(accuracy_val))\n",
    "\n",
    "# Prediction\n",
    "my_score = np.array([[600, 3.8,1]])\n",
    "scaled_my_score = scaler_x.transform(my_score)\n",
    "\n",
    "result = sess.run(H,feed_dict={X:scaled_my_score})\n",
    "print('####### tensorflow 결과값 #########')\n",
    "print('내 지원정보 : {}, 결과 : {}'.format(my_score,result))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>620</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>3.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0</td>\n",
       "      <td>460</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>3.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "      <td>600</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     admit  gre   gpa  rank\n",
       "0        0  380  3.61     3\n",
       "1        1  660  3.67     3\n",
       "2        1  800  4.00     1\n",
       "3        1  640  3.19     4\n",
       "4        0  520  2.93     4\n",
       "..     ...  ...   ...   ...\n",
       "395      0  620  4.00     2\n",
       "396      0  560  3.04     3\n",
       "397      0  460  2.63     2\n",
       "398      0  700  3.65     2\n",
       "399      0  600  3.89     3\n",
       "\n",
       "[400 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('./data/admission.csv')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env]",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
