{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1UN5hHJmQLH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbbVX1kuuxt-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('/content/drive/My Drive/MachineLearning/train.csv')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z1qssMZs4cvM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        thin       0.50      1.00      0.67         1\n",
      "      normal       0.00      0.00      0.00         1\n",
      "         fat       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.50      0.56      0.49         5\n",
      "weighted avg       0.70      0.60      0.61         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = [0, 1 ,2 ,2 ,2]   # 정답\n",
    "y_pred = [0, 0, 2, 2, 1]   # 우리 model이 예측한 값\n",
    "\n",
    "target_name = ['thin', 'normal', 'fat']\n",
    "\n",
    "print(classification_report(y_true,y_pred,target_names=target_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oQoP651CCLSu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 2]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [2, 0, 2, 2, 0 ,1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "\n",
    "confusion_matrix(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3H3-Q2zBagA6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/MachineLearning/Digit_Recognizer_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ea3bd98f4f17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 1. Raw Data Loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/MachineLearning/Digit_Recognizer_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# display(df.head(), df.shape)   # (42000, 785)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/data_env_tensorflow2/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/data_env_tensorflow2/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/data_env_tensorflow2/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/data_env_tensorflow2/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/data_env_tensorflow2/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/MachineLearning/Digit_Recognizer_train.csv'"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "\n",
    "# Tensorflow 1.15버전을 가지고 MNIST예제를 구현\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns    # confusion matrix를 heatmap을 통해서 그래프 출력   \n",
    "from sklearn.preprocessing import MinMaxScaler   # Normalization\n",
    "from sklearn.model_selection import train_test_split # train, test 분리\n",
    "from sklearn.model_selection import KFold        # Cross Validation\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "\n",
    "# 1. Raw Data Loading\n",
    "df = pd.read_csv('/content/drive/My Drive/MachineLearning/Digit_Recognizer_train.csv')\n",
    "# display(df.head(), df.shape)   # (42000, 785)\n",
    "\n",
    "# 2. 결측치와 이상치 처리\n",
    "#    결측치를 찾고 만약 결측치가 존재하면 수정\n",
    "#    이상치를 찾고 만약 있으면 이상치를 수정(scipy의 zscore이용하면 편해요!)\n",
    "#    결측치와 이상치가 MNIST는 존재하지 않아요!!\n",
    "\n",
    "# 3. 사용하는 데이터가 이미지 데이터예요!!\n",
    "#    어떤 이미지인지 한번 확인하고 갈께요!!\n",
    "#    df에서 label column은 제외하고 pixel데이터만 들고올꺼예요!!\n",
    "img_data = df.drop('label', axis=1, inplace=False).values  \n",
    "     # 이미지들의 pixel 데이터만 ndarray로 추출(2차원)\n",
    "# 이런 이미지 데이터를 화면에 출력\n",
    "fig = plt.figure()  # 출력할 전체 화면을 지칭하는 객체를 가져와요!\n",
    "# fig안에 subplot을 만들꺼예요!! 이 subplot을 저장할 list를 하나 만들어요!!\n",
    "fig_arr = list()\n",
    "\n",
    "for n in range(10):\n",
    "    fig_arr.append(fig.add_subplot(2,5,n+1))\n",
    "    fig_arr[n].imshow(img_data[n].reshape(28,28), cmap='Greys', \n",
    "                      interpolation='nearest')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Data Split\n",
    "#    데이터는 크게 3부분으로 나누어야 해요!\n",
    "#    일단 2부분으로 나누어요! (train용, test용)\n",
    "#    여기서 train용이라고 되어 있는 데이터를 다시 2부분으로 분리(train,validation)\n",
    "#    train : 학습용(반복학습으로 모델을 완성)\n",
    "#    validation : 모델 수정용도의 데이터 셋\n",
    "x_data_train, x_data_test, t_data_train, t_data_test = \\\n",
    "train_test_split(df.drop('label', axis=1), df['label'], test_size=0.3, \n",
    "                 random_state=0)\n",
    "                 \n",
    "# 5. Normalization(x_data, 독립변수의 처리)\n",
    "scaler = MinMaxScaler()   # 객체 생성\n",
    "scaler.fit(x_data_train)  # 객체에 정보를 제공\n",
    "x_data_train_norm = scaler.transform(x_data_train)\n",
    "x_data_test_norm = scaler.transform(x_data_test)\n",
    "\n",
    "del x_data_train   \n",
    "del x_data_test\n",
    "\n",
    "# 6. 지금해결해야 하는 문제가 multinomial이예요!\n",
    "#    t_data(label데이터, 정답)를 one hot 형태로 변형\n",
    "\n",
    "sess = tf.Session()   # Tensorflow node를 실행하기 위해서 session을 생성\n",
    "\n",
    "# depth는 label의 종류 개수\n",
    "t_data_train_onehot = sess.run(tf.one_hot(t_data_train, depth=10))  \n",
    "t_data_test_onehot = sess.run(tf.one_hot(t_data_test, depth=10))\n",
    "\n",
    "###########################################\n",
    "# traing용, test용 데이터가 준비되었어요!!\n",
    "\n",
    "###########################################\n",
    "# Tensorflow 구현\n",
    "\n",
    "# 1. placeholder\n",
    "# X의 의미는 x_data(독립변수)를 받아들이기 위한 placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "T = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "# 2. Weight, bias\n",
    "W = tf.Variable(tf.random.normal([784,10]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([10]), name='bias')\n",
    "\n",
    "# 3. Model(Hypothesis)  => Multinomial\n",
    "logit = tf.matmul(X,W) + b   # Linear Regression\n",
    "H = tf.nn.softmax(logit)     # Multinomial Hypothesis\n",
    "\n",
    "# 4. loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,\n",
    "                                                                 labels=T))\n",
    "\n",
    "# 5. Optimizer를 이용한 train(Optimizer는 loss값을 줄이는 알고리즘)\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-1).minimize(loss)\n",
    "\n",
    "# 6. 학습용 parameter setting (기본적으로 2개는 설정)\n",
    "num_of_epoch = 100\n",
    "batch_size = 100\n",
    "\n",
    "# 7. 학습진행\n",
    "def run_train(sess,train_x, train_t):\n",
    "    print('### 학습 시작 ###')\n",
    "    sess.run(tf.global_variables_initializer())  # tf.Variable 초기화(W,b)\n",
    "\n",
    "    total_batch = int(train_x.shape[0] / batch_size)\n",
    "    for step in range(num_of_epoch):\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "            batch_t = train_t[i*batch_size:(i+1)*batch_size]\n",
    "            _, loss_val = sess.run([train,loss], feed_dict={X:batch_x,\n",
    "                                                            T:batch_t})\n",
    "        if step % 10 == 0:\n",
    "            print('Loss : {}'.format(loss_val))\n",
    "    print('### 학습 끝 ###')\n",
    "\n",
    "# Accuracy\n",
    "predict = tf.argmax(H,1)   # [[0.1 0.3  0.2 0.2 ... 0.1]]\n",
    "\n",
    "# sklearn을 이용해서 classification_report를 출력해보아요!!\n",
    "target_name = ['num 0', 'num 1', 'num 2', 'num 3',\n",
    "               'num 4', 'num 5', 'num 6', 'num 7',\n",
    "               'num 8', 'num 9']\n",
    "# train데이터로 학습하고 train데이터로 성능평가를 해 보아요!!  \n",
    "\n",
    "run_train(sess,x_data_train_norm,t_data_train_onehot)\n",
    "print(classification_report(t_data_train,\n",
    "                            sess.run(predict, feed_dict={X:x_data_train_norm}),\n",
    "                            target_names=target_name))               \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nh0RStKe9vIP"
   },
   "outputs": [],
   "source": [
    "# seaborn을 이용한 confusion matrix의 그래프 출력\n",
    "fig, ax = plt.subplots(figsize=(10,10))  # inch단위로 그림의 크기\n",
    "sns.heatmap(\n",
    "    confusion_matrix(t_data_train,\n",
    "                     sess.run(predict, feed_dict={X:x_data_train_norm})),\n",
    "    annot = True, # 숫자표현\n",
    "    cbar = True,  # color bar \n",
    "    fmt = '3d',    # 정수표현     \n",
    "    cmap = 'Blues', # color 색상\n",
    "    ax = ax       # 그래프로 사용할 subplot\n",
    ")\n",
    "ax.set_xlabel('Predict')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPgYQGqHvZEp"
   },
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow   # 현재 설치된 tensorflow 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pog1F3vx4aT-"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.15 # tensorflow 1.15버전 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xre9F5Ot4Czi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNmHuFqxciuC2LpqyvnN3DS",
   "collapsed_sections": [],
   "mount_file_id": "1qkVmo3woNvn2g21QVrsl-UFefM_0Fdm3",
   "name": "MNIST.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
