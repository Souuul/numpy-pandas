{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9월 28일 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다변수 수치미분코드\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def numerical_derivative(f,x):\n",
    "    \n",
    "    # f : 미분하려고 하는 다변수 함수\n",
    "    # x : 모든 변수를 포함하고 있는 numpy array(차원상관없음)\n",
    "    \n",
    "    delta_x = 1e-4\n",
    "    derivative_x = np.zeros_like(x)  # 계산된 수치미분 값을 저장하기 위한 변수\n",
    "    \n",
    "    # iterator를 이용하여 입력변수 x에 대해 편미분 수행\n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        \n",
    "        idx = it.multi_index   # 현재의 index를 추출 => tuple형태로 리턴\n",
    "        \n",
    "        tmp = x[idx]           # 현재 idx의 값을 잠시 보존. delta_x를 이용한 값으로\n",
    "                               # ndarray를 수정한 후 함수값을 계산해야 하기 때문\n",
    "                               # 함수값을 계산한 후 원상복구해야 다음 변수에 대한 편미분을\n",
    "                               # 정상적으로 수행할 수 있다.\n",
    "        \n",
    "        x[idx] = tmp + delta_x\n",
    "        fx_plus_delta = f(x)   # f(x + delta_x)\n",
    "        \n",
    "        x[idx] = tmp - delta_x\n",
    "        fx_minus_delta = f(x)   # f(x - delta_x)\n",
    "        \n",
    "        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)\n",
    "    \n",
    "        x[idx] = tmp\n",
    "        \n",
    "        it.iternext()\n",
    "        \n",
    "    return derivative_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### python 결과값 #########\n",
      "공부시간 : [[13]], 결과 : (1, array([[0.58032326]]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Logistic Regression을 python, tensorflow, sklearn으로 각각구현해 보아요!\n",
    "# 처음은 독립변수가 1개인 걸로 가요!!\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "\n",
    "# 수치미분함수를 들고와서 사용해요!\n",
    "# ##################################\n",
    "# 다변수 수치미분코드\n",
    "\n",
    "def numerical_derivative(f,x):\n",
    "    \n",
    "    # f : 미분하려고 하는 다변수 함수\n",
    "    # x : 모든 변수를 포함하고 있는 numpy array(차원상관없음)\n",
    "    \n",
    "    delta_x = 1e-4\n",
    "    derivative_x = np.zeros_like(x)  # 계산된 수치미분 값을 저장하기 위한 변수\n",
    "    \n",
    "    # iterator를 이용하여 입력변수 x에 대해 편미분 수행\n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        \n",
    "        idx = it.multi_index   # 현재의 index를 추출 => tuple형태로 리턴\n",
    "        \n",
    "        tmp = x[idx]           # 현재 idx의 값을 잠시 보존. delta_x를 이용한 값으로\n",
    "                               # ndarray를 수정한 후 함수값을 계산해야 하기 때문\n",
    "                               # 함수값을 계산한 후 원상복구해야 다음 변수에 대한 편미분을\n",
    "                               # 정상적으로 수행할 수 있다.\n",
    "        \n",
    "        x[idx] = tmp + delta_x\n",
    "        fx_plus_delta = f(x)   # f(x + delta_x)\n",
    "        \n",
    "        x[idx] = tmp - delta_x\n",
    "        fx_minus_delta = f(x)   # f(x - delta_x)\n",
    "        \n",
    "        derivative_x[idx] = (fx_plus_delta - fx_minus_delta) / (2 * delta_x)\n",
    "    \n",
    "        x[idx] = tmp\n",
    "        \n",
    "        it.iternext()\n",
    "        \n",
    "    return derivative_x\n",
    "\n",
    "# ##################################\n",
    "\n",
    "# Raw Data Loading + Data Preprocessing\n",
    "# 그런데 이번예제는 이 과정이 필요없죠!!\n",
    "\n",
    "# Training Data Set\n",
    "# 지도학습을 하고 있기 때문에 독립변수와 종속변수(label)로 구분해서 데이터를 준비\n",
    "# 어떤경우에는 이 두개를 아예 분리해서 제공하는 경우도 있어요!\n",
    "x_data = np.arange(2,21,2).reshape(-1,1)\n",
    "t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(-1,1)\n",
    "\n",
    "#########################################################\n",
    "# python 구현부터 해 보아요!!\n",
    "\n",
    "# Weight & bias     \n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)   # (1,)\n",
    "\n",
    "# 위에서 정의한 W와 b의 값을 구해야 해요!\n",
    "# 이 값만 구하면 우리의 최종 목적인 model을 완성할 수 있어요!\n",
    "\n",
    "# loss function(손실함수, cost function , 비용함수)\n",
    "# 우리 모델의 예측값과 들어온 t_data(정답)\n",
    "# 입력으로 들어온 x_data와 W,b값을 이용해서 예측값 계산\n",
    "# t_data(정답)을 비교해되요!!\n",
    "def loss_func(input_obj):\n",
    "    \n",
    "    # input_obj : W와 b를 같이 포함하고 있는 ndarray => [W1 W2 W3 b]\n",
    "    num_of_bias = b.shape[0]   # num_of_bias : 1\n",
    "    \n",
    "    input_W = input_obj[:-1*num_of_bias].reshape(-1,num_of_bias)   # 행렬연산을 하기 위한 W를 생성\n",
    "    input_b = input_obj[-1*num_of_bias:]                           # bias\n",
    "    \n",
    "    \n",
    "    #  우리 모델의 예측값 : (linear regression model(Wx + b) ==> sigmoid를 적용 )\n",
    "    z = np.dot(x_data,input_W) + input_b\n",
    "    y = 1 / ( 1 + np.exp(-1 * z) )  # sigmoid\n",
    "    \n",
    "    delta = 1e-7  #  굉장히 작은값을 이용해서 프로그램으로 \n",
    "                  # 로그 연산시 무한대로 발산하는것을 방지\n",
    "        \n",
    "    # cross entropy\n",
    "    return -np.sum(t_data*np.log(y+delta) + ((1-t_data)*np.log(1-y+delta)))\n",
    "    \n",
    "# learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    \n",
    "    input_param = np.concatenate((W.ravel(), b.ravel()),axis=0)   # [W1 W2 W3 b]\n",
    "    derivative_result = learning_rate* numerical_derivative(loss_func,input_param)\n",
    "\n",
    "    num_of_bias = b.shape[0] \n",
    "    \n",
    "    W = W - derivative_result[:-1*num_of_bias].reshape(-1,num_of_bias)   # [[W1] [W2] [W3]]\n",
    "    b = b - derivative_result[-1*num_of_bias:]\n",
    "    \n",
    "    \n",
    "# predict => W,b를 다 구해서!! 우리의 Logistic Regression Model을 완성!!\n",
    "def logistic_predict(x):  # 공부한 시간이 입력으로 들어와요!!\n",
    "    \n",
    "    z = np.dot(x,W) + b\n",
    "    y = 1 / ( 1 + np.exp(-1*z) )\n",
    "    \n",
    "    if y < 0.5:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = 1\n",
    "        \n",
    "    return result, y\n",
    "\n",
    "study_hour = np.array([[13]])\n",
    "result = logistic_predict(study_hour)\n",
    "print('####### python 결과값 #########')\n",
    "print('공부시간 : {}, 결과 : {}'.format(study_hour,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### sklearn 결과값 #########\n",
      "공부시간 : [[13]], 결과 : [0],[[0.50009391 0.49990609]]\n"
     ]
    }
   ],
   "source": [
    "### sklearn ### 으로 구현해보아요!\n",
    "\n",
    "# Logistic Regression Model을 생성해요!\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "# Training data set을 이용해서 학습\n",
    "model.fit(x_data,t_data.ravel())\n",
    "\n",
    "study_hour = np.array([[13]])\n",
    "predict_val = model.predict(study_hour)\n",
    "predict_proba = model.predict_proba(study_hour)\n",
    "print('####### sklearn 결과값 #########')\n",
    "print('공부시간 : {}, 결과 : {},{}'.format(study_hour,predict_val,predict_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### tensorflow 결과값 #########\n",
      "공부시간 : [13], 결과 : [[0.5767814]]\n"
     ]
    }
   ],
   "source": [
    "# tensorflow를 이용한 구현\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(dtype=tf.float32)   # 독립변수가 1개인경우 shape명시하지 않아요! (x_data)\n",
    "T = tf.placeholder(dtype=tf.float32)   # (t_data)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([1,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "logit = W * X + b  # matrix 곱연산 하지 않나요?? \n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    sess.run(train, feed_dict={X:x_data, T:t_data})\n",
    "\n",
    "\n",
    "study_hour = np.array([13])\n",
    "result = sess.run(H,feed_dict={X:study_hour})\n",
    "print('####### tensorflow 결과값 #########')\n",
    "print('공부시간 : {}, 결과 : {}'.format(study_hour,result))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Variable Logistic Regression\n",
    "# 독립변수가 2개 이상인 Logistic Regression\n",
    "\n",
    "# 학습하는 데이터는 GRE(Graduate Record Examination)와 \n",
    "# GPA(Grade Point Average) 성적 그리고 \n",
    "# Rank(University Rating)에 대한 \n",
    "# 대학원 합격/불합격 정보\n",
    "\n",
    "# 내 성적 [600.    3.8   1. ] 의 결과??\n",
    "# 첫번째 구현은 sklearn으로 하세요!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### sklearn으로 구현한 결과 ####\n",
      "[[600.    3.8   1. ]] [1] [[0.43740782 0.56259218]]\n",
      "W : [[ 0.2592851 ]\n",
      " [-0.5046855 ]\n",
      " [ 0.11609091]], b : [1.2572142], loss : 1.0727263689041138\n",
      "W : [[-0.08057044]\n",
      " [-0.8430981 ]\n",
      " [-0.3586386 ]], b : [0.4915542], loss : 0.6762152314186096\n",
      "W : [[-0.12383109]\n",
      " [-0.8641973 ]\n",
      " [-0.5467179 ]], b : [0.26643762], loss : 0.6435897946357727\n",
      "W : [[-0.09584159]\n",
      " [-0.8108591 ]\n",
      " [-0.65887874]], b : [0.17782116], loss : 0.635351836681366\n",
      "W : [[-0.05088537]\n",
      " [-0.74046713]\n",
      " [-0.7460983 ]], b : [0.1269434], loss : 0.6296079158782959\n",
      "W : [[-0.00326049]\n",
      " [-0.6678386 ]\n",
      " [-0.822165  ]], b : [0.08745103], loss : 0.6246463060379028\n",
      "W : [[ 0.04315035]\n",
      " [-0.5973139 ]\n",
      " [-0.8911353 ]], b : [0.05190557], loss : 0.6202628016471863\n",
      "W : [[ 0.08730545]\n",
      " [-0.53002805]\n",
      " [-0.9546055 ]], b : [0.01806351], loss : 0.6163756251335144\n",
      "W : [[ 0.12904361]\n",
      " [-0.46592307]\n",
      " [-1.0133189 ]], b : [-0.01472689], loss : 0.6129170656204224\n",
      "W : [[ 0.16843648]\n",
      " [-0.40488812]\n",
      " [-1.0677017 ]], b : [-0.04665124], loss : 0.6098345518112183\n",
      "####### tensorflow 결과값 #########\n",
      "내 지원정보 : [[600.    3.8   1. ]], 결과 : [[0.4350916]]\n"
     ]
    }
   ],
   "source": [
    "# %reset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/admission.csv')\n",
    "\n",
    "# 결측치 확인\n",
    "# df.isnull().sum() 결측치는 없네~\n",
    "\n",
    "# 이상치를 확인해서 있으면 제거!\n",
    "# fig = plt.figure()\n",
    "# fig_admit = fig.add_subplot(1,4,1)\n",
    "# fig_gre = fig.add_subplot(1,4,2)\n",
    "# fig_gpa = fig.add_subplot(1,4,3)\n",
    "# fig_rank = fig.add_subplot(1,4,4)\n",
    "\n",
    "# fig_admit.boxplot(df['admit'])\n",
    "# fig_gre.boxplot(df['gre'])\n",
    "# fig_gpa.boxplot(df['gpa'])\n",
    "# fig_rank.boxplot(df['rank'])\n",
    "\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# 확인했더니 이상치가 있어요!\n",
    "# 이상치를 제거해요!!\n",
    "zscore_threshold = 2.0\n",
    "\n",
    "for col in df.columns:\n",
    "    outlier = df[col][np.abs(stats.zscore(df[col])) > zscore_threshold]    \n",
    "    df = df.loc[~df[col].isin(outlier)]\n",
    "    \n",
    "\n",
    "# Training Data Set\n",
    "x_data = df.drop('admit', axis=1, inplace=False).values\n",
    "t_data = df['admit'].values.reshape(-1,1)\n",
    "\n",
    "# 정규화를 진행해야 해요!!\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(x_data)\n",
    "norm_x_data = scaler_x.transform(x_data)  # for python, tensorflow\n",
    "\n",
    "# sklearn을 이용한 구현\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(x_data,t_data.ravel())\n",
    "print('#### sklearn으로 구현한 결과 ####')\n",
    "my_score = np.array([[600, 3.8,1]])\n",
    "predict_val = model.predict(my_score)  # 0 or 1\n",
    "predict_proba = model.predict_proba(my_score)  # (불합격할 확률, 합격할 확률)\n",
    "print(my_score, predict_val, predict_proba)\n",
    "\n",
    "# Tensorflow\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32)  # 독립변수의 데이터를 받기위한 placeholder\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)  # 종속변수(label)의 데이터를 받기위한 placeholder\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)\n",
    "\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(300000):\n",
    "    _, W_val, b_val, loss_val = sess.run([train, W, b, loss], \n",
    "                                         feed_dict={X:norm_x_data, T:t_data})\n",
    "    \n",
    "    if step % 30000 == 0:\n",
    "        print('W : {}, b : {}, loss : {}'.format(W_val, b_val, loss_val))\n",
    "\n",
    "\n",
    "my_score = np.array([[600, 3.8,1]])\n",
    "scaled_my_score = scaler_x.transform(my_score)\n",
    "\n",
    "result = sess.run(H,feed_dict={X:scaled_my_score})\n",
    "print('####### tensorflow 결과값 #########')\n",
    "print('내 지원정보 : {}, 결과 : {}'.format(my_score,result))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W : [[1.7435304]\n",
      " [0.9507081]\n",
      " [0.948872 ]], b : [-0.23644167], loss : 1.3270013332366943\n",
      "W : [[1.5287209 ]\n",
      " [0.7256944 ]\n",
      " [0.70768046]], b : [-0.64149207], loss : 0.9705049991607666\n",
      "W : [[1.3711586]\n",
      " [0.5634764]\n",
      " [0.5224   ]], b : [-0.9351609], loss : 0.7776923179626465\n",
      "W : [[1.264868  ]\n",
      " [0.4568285 ]\n",
      " [0.38651037]], b : [-1.1333869], loss : 0.6871258616447449\n",
      "W : [[1.1962118 ]\n",
      " [0.39054546]\n",
      " [0.28652486]], b : [-1.2634977], loss : 0.6465348601341248\n",
      "W : [[1.1528543 ]\n",
      " [0.35118192]\n",
      " [0.21061923]], b : [-1.3485235], loss : 0.6280526518821716\n",
      "W : [[1.12608   ]\n",
      " [0.32935748]\n",
      " [0.15059482]], b : [-1.4042077], loss : 0.6191768646240234\n",
      "W : [[1.1101589 ]\n",
      " [0.31896302]\n",
      " [0.10113869]], b : [-1.4407058], loss : 0.6145332455635071\n",
      "W : [[1.1013713 ]\n",
      " [0.31606227]\n",
      " [0.05884017]], b : [-1.464522], loss : 0.6118003726005554\n",
      "W : [[1.0973384 ]\n",
      " [0.31809726]\n",
      " [0.02148782]], b : [-1.4798793], loss : 0.6099565029144287\n",
      "Accuracy : 0.6780104637145996\n",
      "####### tensorflow 결과값 #########\n",
      "내 지원정보 : [[600.    3.8   1. ]], 결과 : [[0.35069087]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Raw Data Loading\n",
    "df = pd.read_csv('./data/admission.csv')\n",
    "\n",
    "# 결측치 확인\n",
    "# df.isnull().sum() 결측치는 없네~\n",
    "\n",
    "# 이상치를 제거해요!!\n",
    "zscore_threshold = 2.0\n",
    "\n",
    "for col in df.columns:\n",
    "    outlier = df[col][np.abs(stats.zscore(df[col])) > zscore_threshold]    \n",
    "    df = df.loc[~df[col].isin(outlier)]\n",
    "    \n",
    "\n",
    "# Training Data Set\n",
    "x_data = df.drop('admit', axis=1, inplace=False).values\n",
    "t_data = df['admit'].values.reshape(-1,1)\n",
    "\n",
    "# 정규화를 진행해야 해요!!\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_x.fit(x_data)\n",
    "norm_x_data = scaler_x.transform(x_data)  # for python, tensorflow\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32)  # 독립변수의 데이터를 받기위한 placeholder\n",
    "T = tf.placeholder(shape=[None,1], dtype=tf.float32)  # 종속변수(label)의 데이터를 받기위한 placeholder\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
    "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
    "\n",
    "# hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# loss function\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=T))\n",
    "\n",
    "# train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)\n",
    "\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(90000):\n",
    "    _, W_val, b_val, loss_val = sess.run([train, W, b, loss], \n",
    "                                         feed_dict={X:norm_x_data, T:t_data})\n",
    "    \n",
    "    if step % 9000 == 0:\n",
    "        print('W : {}, b : {}, loss : {}'.format(W_val, b_val, loss_val))\n",
    "\n",
    "        \n",
    "# 정확도(Accuracy)측정\n",
    "predict = tf.cast(H >= 0.5, dtype=tf.float32)  # True -> 1.0, False -> 0\n",
    "correct = tf.equal(predict,T)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "accuracy_val = sess.run(accuracy, feed_dict={X:norm_x_data, T:t_data})\n",
    "print('Accuracy : {}'.format(accuracy_val))\n",
    "        \n",
    "# prediction        \n",
    "my_score = np.array([[600, 3.8,1]])\n",
    "scaled_my_score = scaler_x.transform(my_score)\n",
    "\n",
    "result = sess.run(H,feed_dict={X:scaled_my_score})\n",
    "print('####### tensorflow 결과값 #########')\n",
    "print('내 지원정보 : {}, 결과 : {}'.format(my_score,result))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_env] *",
   "language": "python",
   "name": "conda-env-data_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
